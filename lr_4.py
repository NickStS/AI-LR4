# -*- coding: utf-8 -*-
"""LR_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/111dNOT13tyVraINNmZBTTSBSinwhVol8

# Лабораторна робота №3  
**Тема:** Множинна лінійна регресія для оцінювання метрик програмного забезпечення  


## Мета роботи
Набути практичних навичок побудови множинного лінійного рівняння регресії для оцінювання метрик програмного забезпечення (ПЗ), перевірки мультиколінеарності, якості моделі (R², MMRE, PRED(0.25)), аналізу залишків і перевірок статистичних припущень.
"""

# Імпорти та налаштування
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.diagnostic import het_breuschpagan

from scipy import stats

plt.rcParams["figure.figsize"] = (7,5)
plt.rcParams["axes.grid"] = True
plt.rcParams["figure.dpi"] = 120

print("Імпорти готові.")

"""## 1) Дані (завжди синтетичні, CSV користувача не читаємо)"""

# Генеруємо синтетичний набір метрик ПЗ
rng = np.random.default_rng(42)
N = 300

LOC     = rng.normal(800, 250, size=N).clip(50, None)
NOM     = rng.normal(20, 7, size=N).clip(1, None)
CC      = rng.normal(12, 5, size=N).clip(1, None)
FanIn   = rng.normal(4, 2, size=N).clip(0, None)
FanOut  = (FanIn + rng.normal(0, 1.2, size=N)).clip(0, None)
Churn   = rng.normal(100, 60, size=N).clip(0, None)
Age     = rng.normal(18, 8, size=N).clip(1, None)

noise = rng.normal(0, 2.0, size=N)
Defects = (
    0.0035*LOC +
    0.06*NOM +
    0.07*CC +
    0.10*FanOut +
    0.015*Churn +
    0.02*Age +
    1.2 + noise
)

df = pd.DataFrame({
    "LOC": LOC, "NOM": NOM, "CC": CC, "FanIn": FanIn, "FanOut": FanOut,
    "Churn": Churn, "Age": Age, "Defects": Defects
})

df_path = "software_metrics_synthetic.csv"
df.to_csv(df_path, index=False)
print("Збережено датасет:", df_path)
df.head()

"""## 2) Перевірка мультиколінеарності (VIF) та відсікання факторів"""

# Ціль і фактори
y = df["Defects"]
X = df.drop(columns=["Defects"]).copy()

# Функція VIF
def compute_vif(design: pd.DataFrame) -> pd.DataFrame:
    out = []
    for i, col in enumerate(design.columns):
        out.append((col, variance_inflation_factor(design.values, i)))
    return pd.DataFrame(out, columns=["feature","VIF"]).sort_values("VIF", ascending=False)

# Стандартизація для стабільного VIF
Xs = (X - X.mean())/X.std(ddof=0)
Xs = Xs.fillna(0.0)

vif_table = compute_vif(Xs)
display(vif_table)

# Відсікаємо фактори з VIF > 10
THRESH = 10.0
for _ in range(10):
    worst = vif_table.iloc[0]
    if worst["VIF"] <= THRESH:
        break
    Xs = Xs.drop(columns=[worst["feature"]])
    X  = X.drop(columns=[worst["feature"]])
    vif_table = compute_vif(Xs)

print("Фінальний перелік факторів:", list(X.columns))
vif_final = compute_vif(((X - X.mean())/X.std(ddof=0)).fillna(0.0))
vif_final

"""## 3) Оцінка параметрів (OLS), T-тести, F-тест"""

X_sm = sm.add_constant(X)
ols = sm.OLS(y, X_sm).fit()
display(ols.summary())

lin = LinearRegression().fit(X, y)
y_hat = lin.predict(X)

print("F-тест значущості моделі: F =", round(ols.fvalue, 3), ", p =", f"{ols.f_pvalue:.3g}")

"""## 4) Метрики якості: R², Adjusted R², MMRE, PRED(0.25)"""

R2 = r2_score(y, y_hat)
n, k = X.shape
adj_R2 = 1 - (1 - R2) * (n - 1) / (n - k - 1)

resid = y - y_hat
with np.errstate(divide='ignore', invalid='ignore'):
    MRE = np.abs(resid) / np.where(y != 0, np.abs(y), np.nan)
MMRE = np.nanmean(MRE)
PRED025 = np.nanmean((MRE <= 0.25).astype(float)) * 100.0

print("R^2:", round(R2, 4))
print("Adjusted R^2:", round(adj_R2, 4))
print("MMRE:", round(MMRE, 4))
print("PRED(0.25):", f"{PRED025:.2f}%")

"""## 5) Перевірка припущень: нормальність і гетероскедастичність"""

# Нормальність залишків
k2_stat, k2_p = stats.normaltest(resid)
sw_stat, sw_p = stats.shapiro(resid)

# Гетероскедастичність
bp_stat, bp_p, _, _ = het_breuschpagan(resid, X_sm)

print("D’Agostino K^2: stat =", round(k2_stat,3), ", p =", f"{k2_p:.3g}")
print("Shapiro–Wilk:   stat =", round(sw_stat,3), ", p =", f"{sw_p:.3g}")
print("Breusch–Pagan:  stat =", round(bp_stat,3), ", p =", f"{bp_p:.3g}")

"""## 6) Візуалізація залишків"""

plt.figure()
plt.hist(resid, bins=30)
plt.title("Гістограма залишків")
plt.xlabel("Залишок")
plt.ylabel("Частота")
plt.tight_layout()
plt.show()

fig = sm.qqplot(resid, line='45', fit=True)
plt.title("Q–Q графік залишків")
plt.tight_layout()
plt.show()

plt.figure()
plt.scatter(y_hat, resid, s=18)
plt.axhline(0, linestyle="--")
plt.title("Залишки vs Прогнози")
plt.xlabel("Прогнозовані значення (Ŷ)")
plt.ylabel("Залишок (ε)")
plt.tight_layout()
plt.show()

"""
## 7) Висновки
- Модель множинної лінійної регресії побудована для цільової змінної **Defects** за фінальним набором факторів (після VIF-відсікання).  
- Значущість моделі підтверджується F-тестом; значущість коефіцієнтів — T-тестами (див. `summary()`).  
- Якість моделі оцінено показниками R², Adjusted R², MMRE, PRED(0.25).  
- Перевірено припущення про нормальність та однорідність дисперсії залишків.  
- За потреби — можливі поліпшення (лог-/Box-Cox-трансформації, робастні підходи, видалення викидів, регуляризація).
"""